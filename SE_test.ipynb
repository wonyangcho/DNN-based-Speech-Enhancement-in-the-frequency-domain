{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37dada11-858a-4c37-be37-1f48408594ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from pesq import pesq\n",
    "import torch.nn as nn\n",
    "from pystoi import stoi\n",
    "from scipy import interpolate\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import soundfile\n",
    "from scipy.signal import get_window\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247cdc7c-6057-4ec5-a0a4-d7ae18542cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913b62b-79aa-4006-9c56-9b03af2852ef",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b79584-e9d6-4110-95f6-436b9abb89eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  C  O  N  F  I  G  ----------------------\n",
      "--------------------------------------------------------------\n",
      "MODEL INFO : DCCRN\n",
      "LOSS INFO : SDR, perceptual : False\n",
      "LSTM : complex\n",
      "SKIP : True\n",
      "MASKING INFO : E\n",
      "\n",
      "BATCH : 10\n",
      "LEARNING RATE : 0.001\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration for train_interface\n",
    "\n",
    "You can check the essential information,\n",
    "and if you want to change model structure or training method,\n",
    "you have to change this file.\n",
    "\"\"\"\n",
    "#######################################################################\n",
    "#                                 path                                #\n",
    "#######################################################################\n",
    "job_dir = './models/'\n",
    "logs_dir = './logs/'\n",
    "chkpt_model = None  # 'FILE PATH (if you have pretrained model..)'\n",
    "chkpt = str(\"EPOCH\")\n",
    "if chkpt_model is not None:\n",
    "    chkpt_path = job_dir + chkpt_model + '/chkpt_' + chkpt + '.pt'\n",
    "\n",
    "#######################################################################\n",
    "#                         possible setting                            #\n",
    "#######################################################################\n",
    "# the list you can do\n",
    "model_list = ['DCCRN', 'CRN', 'FullSubNet']\n",
    "loss_list = ['MSE', 'SDR', 'SI-SNR', 'SI-SDR']\n",
    "perceptual_list = [False, 'LMS', 'PMSQE']\n",
    "lstm_type = ['real', 'complex']\n",
    "main_net = ['LSTM', 'GRU']\n",
    "mask_type = ['Direct(None make)', 'E', 'C', 'R']\n",
    "\n",
    "# experiment number setting\n",
    "expr_num = 'EXPERIMENT_NUMBER'\n",
    "DEVICE = 'cuda'  # if you want to run the code with 'cpu', change 'cpu'\n",
    "#######################################################################\n",
    "#                           current setting                           #\n",
    "#######################################################################\n",
    "model = model_list[0]\n",
    "loss = loss_list[1]\n",
    "perceptual = perceptual_list[0]\n",
    "lstm = lstm_type[1]\n",
    "sequence_model = main_net[0]\n",
    "\n",
    "masking_mode = mask_type[1]\n",
    "skip_type = True   # False, if you want to remove 'skip connection'\n",
    "\n",
    "# hyper-parameters\n",
    "max_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch = 10\n",
    "\n",
    "# kernel size\n",
    "dccrn_kernel_num = [32, 64, 128, 256, 256, 256]\n",
    "#######################################################################\n",
    "#                         model information                           #\n",
    "#######################################################################\n",
    "fs = 8000\n",
    "win_len = 400\n",
    "win_inc = 100\n",
    "ola_ratio = 0.75\n",
    "fft_len = 512\n",
    "sam_sec = fft_len / fs\n",
    "frm_samp = fs * (fft_len / fs)\n",
    "#window = 'hanning' #수정\n",
    "window = 'hann'\n",
    "\n",
    "# for DCCRN\n",
    "rnn_layers = 2\n",
    "rnn_units = 256\n",
    "\n",
    "# for CRN\n",
    "rnn_input_size = 512\n",
    "\n",
    "# for FullSubNet\n",
    "sb_num_neighbors = 15\n",
    "fb_num_neighbors = 0\n",
    "num_freqs = fft_len // 2 + 1\n",
    "look_ahead = 2\n",
    "fb_output_activate_function = \"ReLU\"\n",
    "sb_output_activate_function = None\n",
    "fb_model_hidden_size = 512\n",
    "sb_model_hidden_size = 384\n",
    "weight_init = False\n",
    "norm_type = \"offline_laplace_norm\"\n",
    "num_groups_in_drop_band = 2\n",
    "\n",
    "\n",
    "HOME_DIR =\"/work/wycho/project/DNN-based-Speech-Enhancement-in-the-frequency-domain/\"\n",
    "\n",
    "#######################################################################\n",
    "#                      setting error check                            #\n",
    "#######################################################################\n",
    "# if the setting is wrong, print error message\n",
    "assert not (masking_mode == 'Direct(None make)' and perceptual is not False), \\\n",
    "    \"This setting is not created \"\n",
    "assert not (model == 'FullSubNet' and perceptual is not False), \\\n",
    "    \"This setting is not created \"\n",
    "\n",
    "#######################################################################\n",
    "#                           print setting                             #\n",
    "#######################################################################\n",
    "print('--------------------  C  O  N  F  I  G  ----------------------')\n",
    "print('--------------------------------------------------------------')\n",
    "print('MODEL INFO : {}'.format(model))\n",
    "print('LOSS INFO : {}, perceptual : {}'.format(loss, perceptual))\n",
    "if model != 'FullSubNet':\n",
    "    print('LSTM : {}'.format(lstm))\n",
    "    print('SKIP : {}'.format(skip_type))\n",
    "    print('MASKING INFO : {}'.format(masking_mode))\n",
    "else:\n",
    "    print('Main network : {}'.format(sequence_model))\n",
    "print('\\nBATCH : {}'.format(batch))\n",
    "print('LEARNING RATE : {}'.format(learning_rate))\n",
    "print('--------------------------------------------------------------')\n",
    "print('--------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec054b5-9505-4f4a-8571-d27989e59281",
   "metadata": {},
   "source": [
    "### Tools for Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0484dbba-af67-44fd-9c04-2f0a7f53d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#               for model structure & loss function                        #\n",
    "############################################################################\n",
    "def remove_dc(data):\n",
    "    mean = torch.mean(data, -1, keepdim=True)\n",
    "    data = data - mean\n",
    "    return data\n",
    "\n",
    "\n",
    "def l2_norm(s1, s2):\n",
    "    # norm = torch.sqrt(torch.sum(s1*s2, 1, keepdim=True))\n",
    "    # norm = torch.norm(s1*s2, 1, keepdim=True)\n",
    "\n",
    "    norm = torch.sum(s1 * s2, -1, keepdim=True)\n",
    "    return norm\n",
    "\n",
    "\n",
    "def sdr(s1, s2, eps=1e-8):\n",
    "    sn = l2_norm(s1, s1)\n",
    "    sn_m_shn = l2_norm(s1 - s2, s1 - s2)\n",
    "    sdr_loss = 10 * torch.log10(sn**2 / (sn_m_shn**2 + eps))\n",
    "    return torch.mean(sdr_loss)\n",
    "\n",
    "\n",
    "def si_snr(s1, s2, eps=1e-8):\n",
    "    # s1 = remove_dc(s1)\n",
    "    # s2 = remove_dc(s2)\n",
    "    s1_s2_norm = l2_norm(s1, s2)\n",
    "    s2_s2_norm = l2_norm(s2, s2)\n",
    "    s_target = s1_s2_norm / (s2_s2_norm + eps) * s2\n",
    "    e_nosie = s1 - s_target\n",
    "    target_norm = l2_norm(s_target, s_target)\n",
    "    noise_norm = l2_norm(e_nosie, e_nosie)\n",
    "    snr = 10 * torch.log10((target_norm) / (noise_norm + eps) + eps)\n",
    "    return torch.mean(snr)\n",
    "\n",
    "\n",
    "def si_sdr(reference, estimation, eps=1e-8):\n",
    "    \"\"\"\n",
    "        Scale-Invariant Signal-to-Distortion Ratio (SI-SDR)\n",
    "        Args:\n",
    "            reference: numpy.ndarray, [..., T]\n",
    "            estimation: numpy.ndarray, [..., T]\n",
    "        Returns:\n",
    "            SI-SDR\n",
    "        [1] SDR– Half- Baked or Well Done?\n",
    "        http://www.merl.com/publications/docs/TR2019-013.pdf\n",
    "        >>> np.random.seed(0)\n",
    "        >>> reference = np.random.randn(100)\n",
    "        >>> si_sdr(reference, reference)\n",
    "        inf\n",
    "        >>> si_sdr(reference, reference * 2)\n",
    "        inf\n",
    "        >>> si_sdr(reference, np.flip(reference))\n",
    "        -25.127672346460717\n",
    "        >>> si_sdr(reference, reference + np.flip(reference))\n",
    "        0.481070445785553\n",
    "        >>> si_sdr(reference, reference + 0.5)\n",
    "        6.3704606032577304\n",
    "        >>> si_sdr(reference, reference * 2 + 1)\n",
    "        6.3704606032577304\n",
    "        >>> si_sdr([1., 0], [0., 0])  # never predict only zeros\n",
    "        nan\n",
    "        >>> si_sdr([reference, reference], [reference * 2 + 1, reference * 1 + 0.5])\n",
    "        array([6.3704606, 6.3704606])\n",
    "        :param reference:\n",
    "        :param estimation:\n",
    "        :param eps:\n",
    "        \"\"\"\n",
    "\n",
    "    reference_energy = torch.sum(reference ** 2, axis=-1, keepdims=True)\n",
    "\n",
    "    # This is $\\alpha$ after Equation (3) in [1].\n",
    "    optimal_scaling = torch.sum(reference * estimation, axis=-1, keepdims=True) / reference_energy + eps\n",
    "\n",
    "    # This is $e_{\\text{target}}$ in Equation (4) in [1].\n",
    "    projection = optimal_scaling * reference\n",
    "\n",
    "    # This is $e_{\\text{res}}$ in Equation (4) in [1].\n",
    "    noise = estimation - projection\n",
    "\n",
    "    ratio = torch.sum(projection ** 2, axis=-1) / torch.sum(noise ** 2, axis=-1) + eps\n",
    "\n",
    "    ratio = torch.mean(ratio)\n",
    "    return 10 * torch.log10(ratio + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574f09d-9d22-4e1c-a01d-036382de3633",
   "metadata": {},
   "source": [
    "### Tools for Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4469ec62-ab3b-4cd2-bdb0-9083ec3cd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                           PESQ (another ref)                                #\n",
    "###############################################################################\n",
    "# interface to PESQ evaluation, taking in two waveforms as input\n",
    "def cal_pesq(dirty_wavs, clean_wavs):\n",
    "    scores = []\n",
    "    for i in range(len(dirty_wavs)):\n",
    "        try:\n",
    "            pesq_score = pesq(fs, dirty_wavs[i], clean_wavs[i], 'nb')\n",
    "            scores.append(pesq_score)\n",
    "        except:\n",
    "            scores.append(5.0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                                     STOI                                    #\n",
    "###############################################################################\n",
    "def cal_stoi(estimated_speechs, clean_speechs):\n",
    "    stoi_scores = []\n",
    "    for i in range(len(estimated_speechs)):\n",
    "        stoi_score = stoi(clean_speechs[i], estimated_speechs[i], fs, extended=False)\n",
    "        stoi_scores.append(stoi_score)\n",
    "    return stoi_scores\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                                     SNR                                     #\n",
    "###############################################################################\n",
    "def cal_snr(s1, s2, eps=1e-8):\n",
    "    signal = s2\n",
    "    mean_signal = np.mean(signal)\n",
    "    signal_diff = signal - mean_signal\n",
    "    var_signal = np.sum(np.mean(signal_diff ** 2))  # # variance of orignal data\n",
    "\n",
    "    noisy_signal = s1\n",
    "    noise = noisy_signal - signal\n",
    "    mean_noise = np.mean(noise)\n",
    "    noise_diff = noise - mean_noise\n",
    "    var_noise = np.sum(np.mean(noise_diff ** 2))  # # variance of noise\n",
    "\n",
    "    if var_noise == 0:\n",
    "        snr_score = 100  # # clean\n",
    "    else:\n",
    "        snr_score = (np.log10(var_signal/var_noise + eps))*10\n",
    "    return snr_score\n",
    "\n",
    "\n",
    "def cal_snr_array(estimated_speechs, clean_speechs):\n",
    "    snr_score = []\n",
    "    for i in range(len(estimated_speechs)):\n",
    "        snr = cal_snr(estimated_speechs[i], clean_speechs[i])\n",
    "        snr_score.append(snr)\n",
    "    return snr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471e064-de38-4f31-ae04-4bdb4a534042",
   "metadata": {},
   "source": [
    "### Tools for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61b53b9-e704-4e98-a5c5-67c5301ef333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_kernels(win_len, win_inc, fft_len, win_type=None, invers=False):\n",
    "    if win_type == 'None' or win_type is None:\n",
    "        window = np.ones(win_len)\n",
    "    else:\n",
    "        window = get_window(win_type, win_len, fftbins=True)  # **0.5\n",
    "\n",
    "    N = fft_len\n",
    "    fourier_basis = np.fft.rfft(np.eye(N))[:win_len]\n",
    "    real_kernel = np.real(fourier_basis)\n",
    "    imag_kernel = np.imag(fourier_basis)\n",
    "    kernel = np.concatenate([real_kernel, imag_kernel], 1).T\n",
    "\n",
    "    if invers:\n",
    "        kernel = np.linalg.pinv(kernel).T\n",
    "\n",
    "    kernel = kernel * window\n",
    "    kernel = kernel[:, None, :]\n",
    "    return torch.from_numpy(kernel.astype(np.float32)), torch.from_numpy(window[None, :, None].astype(np.float32))\n",
    "\n",
    "\n",
    "class ConvSTFT(nn.Module):\n",
    "\n",
    "    def __init__(self, win_len, win_inc, fft_len=None, win_type='hamming', feature_type='real', fix=True):\n",
    "        super(ConvSTFT, self).__init__()\n",
    "\n",
    "        if fft_len == None:\n",
    "            self.fft_len = np.int(2 ** np.ceil(np.log2(win_len)))\n",
    "        else:\n",
    "            self.fft_len = fft_len\n",
    "\n",
    "        kernel, _ = init_kernels(win_len, win_inc, self.fft_len, win_type)\n",
    "        # self.weight = nn.Parameter(kernel, requires_grad=(not fix))\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.feature_type = feature_type\n",
    "        self.stride = win_inc\n",
    "        self.win_len = win_len\n",
    "        self.dim = self.fft_len\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs.dim() == 2:\n",
    "            inputs = torch.unsqueeze(inputs, 1)\n",
    "        inputs = F.pad(inputs, [self.win_len - self.stride, self.win_len - self.stride])\n",
    "        outputs = F.conv1d(inputs, self.weight, stride=self.stride)\n",
    "\n",
    "        if self.feature_type == 'complex':\n",
    "            return outputs\n",
    "        else:\n",
    "            dim = self.dim // 2 + 1\n",
    "            real = outputs[:, :dim, :]\n",
    "            imag = outputs[:, dim:, :]\n",
    "            mags = torch.sqrt(real ** 2 + imag ** 2)\n",
    "            phase = torch.atan2(imag, real)\n",
    "            return mags, phase\n",
    "\n",
    "\n",
    "class ConviSTFT(nn.Module):\n",
    "\n",
    "    def __init__(self, win_len, win_inc, fft_len=None, win_type='hamming', feature_type='real', fix=True):\n",
    "        super(ConviSTFT, self).__init__()\n",
    "        if fft_len == None:\n",
    "            self.fft_len = np.int(2 ** np.ceil(np.log2(win_len)))\n",
    "        else:\n",
    "            self.fft_len = fft_len\n",
    "        kernel, window = init_kernels(win_len, win_inc, self.fft_len, win_type, invers=True)\n",
    "        # self.weight = nn.Parameter(kernel, requires_grad=(not fix))\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.feature_type = feature_type\n",
    "        self.win_type = win_type\n",
    "        self.win_len = win_len\n",
    "        self.stride = win_inc\n",
    "        self.dim = self.fft_len\n",
    "        self.register_buffer('window', window)\n",
    "        self.register_buffer('enframe', torch.eye(win_len)[:, None, :])\n",
    "\n",
    "    def forward(self, inputs, phase=None):\n",
    "        \"\"\"\n",
    "        inputs : [B, N+2, T] (complex spec) or [B, N//2+1, T] (mags)\n",
    "        phase: [B, N//2+1, T] (if not none)\n",
    "        \"\"\"\n",
    "\n",
    "        if phase is not None:\n",
    "            real = inputs * torch.cos(phase)\n",
    "            imag = inputs * torch.sin(phase)\n",
    "            inputs = torch.cat([real, imag], 1)\n",
    "\n",
    "        outputs = F.conv_transpose1d(inputs, self.weight, stride=self.stride)\n",
    "\n",
    "        # this is from torch-stft: https://github.com/pseeth/torch-stft\n",
    "        t = self.window.repeat(1, 1, inputs.size(-1)) ** 2\n",
    "        coff = F.conv_transpose1d(t, self.enframe, stride=self.stride)\n",
    "\n",
    "        outputs = outputs / (coff + 1e-8)\n",
    "\n",
    "        # # outputs = torch.where(coff == 0, outputs, outputs/coff)\n",
    "        outputs = outputs[..., self.win_len - self.stride:-(self.win_len - self.stride)]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                             for complex rnn                              #\n",
    "############################################################################\n",
    "def get_casual_padding1d():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_casual_padding2d():\n",
    "    pass\n",
    "\n",
    "\n",
    "class cPReLU(nn.Module):\n",
    "\n",
    "    def __init__(self, complex_axis=1):\n",
    "        super(cPReLU, self).__init__()\n",
    "        self.r_prelu = nn.PReLU()\n",
    "        self.i_prelu = nn.PReLU()\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        real, imag = torch.chunk(inputs, 2, self.complex_axis)\n",
    "        real = self.r_prelu(real)\n",
    "        imag = self.i_prelu(imag)\n",
    "        return torch.cat([real, imag], self.complex_axis)\n",
    "\n",
    "\n",
    "class NavieComplexLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, projection_dim=None, bidirectional=False, batch_first=False):\n",
    "        super(NavieComplexLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_size // 2\n",
    "        self.rnn_units = hidden_size // 2\n",
    "        self.real_lstm = nn.LSTM(self.input_dim, self.rnn_units, num_layers=1, bidirectional=bidirectional,\n",
    "                                 batch_first=False)\n",
    "        self.imag_lstm = nn.LSTM(self.input_dim, self.rnn_units, num_layers=1, bidirectional=bidirectional,\n",
    "                                 batch_first=False)\n",
    "        if bidirectional:\n",
    "            bidirectional = 2\n",
    "        else:\n",
    "            bidirectional = 1\n",
    "        if projection_dim is not None:\n",
    "            self.projection_dim = projection_dim // 2\n",
    "            self.r_trans = nn.Linear(self.rnn_units * bidirectional, self.projection_dim)\n",
    "            self.i_trans = nn.Linear(self.rnn_units * bidirectional, self.projection_dim)\n",
    "        else:\n",
    "            self.projection_dim = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs, list):\n",
    "            real, imag = inputs\n",
    "        elif isinstance(inputs, torch.Tensor):\n",
    "            real, imag = torch.chunk(inputs, -1)\n",
    "        r2r_out = self.real_lstm(real)[0]\n",
    "        r2i_out = self.imag_lstm(real)[0]\n",
    "        i2r_out = self.real_lstm(imag)[0]\n",
    "        i2i_out = self.imag_lstm(imag)[0]\n",
    "        real_out = r2r_out - i2i_out\n",
    "        imag_out = i2r_out + r2i_out\n",
    "        if self.projection_dim is not None:\n",
    "            real_out = self.r_trans(real_out)\n",
    "            imag_out = self.i_trans(imag_out)\n",
    "        # print(real_out.shape,imag_out.shape)\n",
    "        return [real_out, imag_out]\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        self.imag_lstm.flatten_parameters()\n",
    "        self.real_lstm.flatten_parameters()\n",
    "\n",
    "\n",
    "def complex_cat(inputs, axis):\n",
    "    real, imag = [], []\n",
    "    for idx, data in enumerate(inputs):\n",
    "        r, i = torch.chunk(data, 2, axis)\n",
    "        real.append(r)\n",
    "        imag.append(i)\n",
    "    real = torch.cat(real, axis)\n",
    "    imag = torch.cat(imag, axis)\n",
    "    outputs = torch.cat([real, imag], axis)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                         for convolutional layer                          #\n",
    "############################################################################\n",
    "class ComplexConv2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            causal=True,\n",
    "            complex_axis=1,\n",
    "    ):\n",
    "        '''\n",
    "            in_channels: real+imag\n",
    "            out_channels: real+imag\n",
    "            kernel_size : input [B,C,D,T] kernel size in [D,T]\n",
    "            padding : input [B,C,D,T] padding in [D,T]\n",
    "            causal: if causal, will padding time dimension's left side,\n",
    "                    otherwise both\n",
    "\n",
    "        '''\n",
    "        super(ComplexConv2d, self).__init__()\n",
    "        self.in_channels = in_channels // 2\n",
    "        self.out_channels = out_channels // 2\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.causal = causal\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "        self.real_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                                   padding=[self.padding[0], 0], dilation=self.dilation, groups=self.groups)\n",
    "        self.imag_conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                                   padding=[self.padding[0], 0], dilation=self.dilation, groups=self.groups)\n",
    "\n",
    "        nn.init.normal_(self.real_conv.weight.data, std=0.05)\n",
    "        nn.init.normal_(self.imag_conv.weight.data, std=0.05)\n",
    "        nn.init.constant_(self.real_conv.bias, 0.)\n",
    "        nn.init.constant_(self.imag_conv.bias, 0.)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.padding[1] != 0 and self.causal:\n",
    "            inputs = F.pad(inputs, [self.padding[1], 0, 0, 0])  # # [width left, width right, height left, height right]\n",
    "        else:\n",
    "            inputs = F.pad(inputs, [self.padding[1], self.padding[1], 0, 0])\n",
    "\n",
    "        if self.complex_axis == 0:\n",
    "            real = self.real_conv(inputs)\n",
    "            imag = self.imag_conv(inputs)\n",
    "            real2real, imag2real = torch.chunk(real, 2, self.complex_axis)\n",
    "            real2imag, imag2imag = torch.chunk(imag, 2, self.complex_axis)\n",
    "\n",
    "        else:\n",
    "            if isinstance(inputs, torch.Tensor):\n",
    "                real, imag = torch.chunk(inputs, 2, self.complex_axis)\n",
    "\n",
    "            real2real = self.real_conv(real, )\n",
    "            imag2imag = self.imag_conv(imag, )\n",
    "\n",
    "            real2imag = self.imag_conv(real)\n",
    "            imag2real = self.real_conv(imag)\n",
    "\n",
    "        real = real2real - imag2imag\n",
    "        imag = real2imag + imag2real\n",
    "        out = torch.cat([real, imag], self.complex_axis)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ComplexConvTranspose2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            output_padding=(0, 0),\n",
    "            causal=False,\n",
    "            complex_axis=1,\n",
    "            groups=1\n",
    "    ):\n",
    "        '''\n",
    "            in_channels: real+imag\n",
    "            out_channels: real+imag\n",
    "        '''\n",
    "        super(ComplexConvTranspose2d, self).__init__()\n",
    "        self.in_channels = in_channels // 2\n",
    "        self.out_channels = out_channels // 2\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "\n",
    "        self.real_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                                            padding=self.padding, output_padding=output_padding, groups=self.groups)\n",
    "        self.imag_conv = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                                            padding=self.padding, output_padding=output_padding, groups=self.groups)\n",
    "\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "        nn.init.normal_(self.real_conv.weight.data, std=0.05)\n",
    "        nn.init.normal_(self.imag_conv.weight.data, std=0.05)\n",
    "        nn.init.constant_(self.real_conv.bias, 0.)\n",
    "        nn.init.constant_(self.imag_conv.bias, 0.)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if isinstance(inputs, torch.Tensor):\n",
    "            real, imag = torch.chunk(inputs, 2, self.complex_axis)\n",
    "        elif isinstance(inputs, tuple) or isinstance(inputs, list):\n",
    "            real = inputs[0]\n",
    "            imag = inputs[1]\n",
    "        if self.complex_axis == 0:\n",
    "            real = self.real_conv(inputs)\n",
    "            imag = self.imag_conv(inputs)\n",
    "            real2real, imag2real = torch.chunk(real, 2, self.complex_axis)\n",
    "            real2imag, imag2imag = torch.chunk(imag, 2, self.complex_axis)\n",
    "\n",
    "        else:\n",
    "            if isinstance(inputs, torch.Tensor):\n",
    "                real, imag = torch.chunk(inputs, 2, self.complex_axis)\n",
    "\n",
    "            real2real = self.real_conv(real, )\n",
    "            imag2imag = self.imag_conv(imag, )\n",
    "\n",
    "            real2imag = self.imag_conv(real)\n",
    "            imag2real = self.real_conv(imag)\n",
    "\n",
    "        real = real2real - imag2imag\n",
    "        imag = real2imag + imag2real\n",
    "        out = torch.cat([real, imag], self.complex_axis)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RealConv2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            causal=True,\n",
    "            complex_axis=1,\n",
    "    ):\n",
    "        '''\n",
    "            in_channels: real+imag\n",
    "            out_channels: real+imag\n",
    "            kernel_size : input [B,C,D,T] kernel size in [D,T]\n",
    "            padding : input [B,C,D,T] padding in [D,T]\n",
    "            causal: if causal, will padding time dimension's left side,\n",
    "                    otherwise both\n",
    "\n",
    "        '''\n",
    "        super(RealConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.causal = causal\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                              padding=[self.padding[0], 0], dilation=self.dilation, groups=self.groups)\n",
    "\n",
    "        nn.init.normal_(self.conv.weight.data, std=0.05)\n",
    "        nn.init.constant_(self.conv.bias, 0.)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.padding[1] != 0 and self.causal:\n",
    "            inputs = F.pad(inputs, [self.padding[1], 0, 0, 0])  ## [width left, width right, height left, height right]\n",
    "        else:\n",
    "            inputs = F.pad(inputs, [self.padding[1], self.padding[1], 0, 0])\n",
    "\n",
    "        out = self.conv(inputs)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RealConvTranspose2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            output_padding=(0, 0),\n",
    "            groups=1\n",
    "    ):\n",
    "        '''\n",
    "            in_channels: real+imag\n",
    "            out_channels: real+imag\n",
    "        '''\n",
    "        super(RealConvTranspose2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "\n",
    "        self.conv = nn.ConvTranspose2d(self.in_channels, self.out_channels, kernel_size, self.stride,\n",
    "                                       padding=self.padding, output_padding=output_padding, groups=self.groups)\n",
    "\n",
    "        nn.init.normal_(self.conv.weight.data, std=0.05)\n",
    "        nn.init.constant_(self.conv.bias, 0.)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Source: https://github.com/ChihebTrabelsi/deep_complex_networks/tree/pytorch\n",
    "# from https://github.com/IMLHF/SE_DCUNet/blob/f28bf1661121c8901ad38149ea827693f1830715/models/layers/complexnn.py#L55\n",
    "class ComplexBatchNorm(torch.nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n",
    "                 track_running_stats=True, complex_axis=1):\n",
    "        super(ComplexBatchNorm, self).__init__()\n",
    "        self.num_features = num_features // 2\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.complex_axis = complex_axis\n",
    "\n",
    "        if self.affine:\n",
    "            self.Wrr = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wri = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Wii = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Br = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "            self.Bi = torch.nn.Parameter(torch.Tensor(self.num_features))\n",
    "        else:\n",
    "            self.register_parameter('Wrr', None)\n",
    "            self.register_parameter('Wri', None)\n",
    "            self.register_parameter('Wii', None)\n",
    "            self.register_parameter('Br', None)\n",
    "            self.register_parameter('Bi', None)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.register_buffer('RMr', torch.zeros(self.num_features))\n",
    "            self.register_buffer('RMi', torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVrr', torch.ones(self.num_features))\n",
    "            self.register_buffer('RVri', torch.zeros(self.num_features))\n",
    "            self.register_buffer('RVii', torch.ones(self.num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_parameter('RMr', None)\n",
    "            self.register_parameter('RMi', None)\n",
    "            self.register_parameter('RVrr', None)\n",
    "            self.register_parameter('RVri', None)\n",
    "            self.register_parameter('RVii', None)\n",
    "            self.register_parameter('num_batches_tracked', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_running_stats(self):\n",
    "        if self.track_running_stats:\n",
    "            self.RMr.zero_()\n",
    "            self.RMi.zero_()\n",
    "            self.RVrr.fill_(1)\n",
    "            self.RVri.zero_()\n",
    "            self.RVii.fill_(1)\n",
    "            self.num_batches_tracked.zero_()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            self.Br.data.zero_()\n",
    "            self.Bi.data.zero_()\n",
    "            self.Wrr.data.fill_(1)\n",
    "            self.Wri.data.uniform_(-.9, +.9)  # W will be positive-definite\n",
    "            self.Wii.data.fill_(1)\n",
    "\n",
    "    def _check_input_dim(self, xr, xi):\n",
    "        assert (xr.shape == xi.shape)\n",
    "        assert (xr.size(1) == self.num_features)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # self._check_input_dim(xr, xi)\n",
    "\n",
    "        xr, xi = torch.chunk(inputs, 2, axis=self.complex_axis)\n",
    "        exponential_average_factor = 0.0\n",
    "\n",
    "        if self.training and self.track_running_stats:\n",
    "            self.num_batches_tracked += 1\n",
    "            if self.momentum is None:  # use cumulative moving average\n",
    "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
    "            else:  # use exponential moving average\n",
    "                exponential_average_factor = self.momentum\n",
    "\n",
    "        #\n",
    "        # NOTE: The precise meaning of the \"training flag\" is:\n",
    "        #       True:  Normalize using batch   statistics, update running statistics\n",
    "        #              if they are being collected.\n",
    "        #       False: Normalize using running statistics, ignore batch   statistics.\n",
    "        #\n",
    "        training = self.training or not self.track_running_stats\n",
    "        redux = [i for i in reversed(range(xr.dim())) if i != 1]\n",
    "        vdim = [1] * xr.dim()\n",
    "        vdim[1] = xr.size(1)\n",
    "\n",
    "        #\n",
    "        # Mean M Computation and Centering\n",
    "        #\n",
    "        # Includes running mean update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Mr, Mi = xr, xi\n",
    "            for d in redux:\n",
    "                Mr = Mr.mean(d, keepdim=True)\n",
    "                Mi = Mi.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RMr.lerp_(Mr.squeeze(), exponential_average_factor)\n",
    "                self.RMi.lerp_(Mi.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Mr = self.RMr.view(vdim)\n",
    "            Mi = self.RMi.view(vdim)\n",
    "        xr, xi = xr - Mr, xi - Mi\n",
    "\n",
    "        #\n",
    "        # Variance Matrix V Computation\n",
    "        #\n",
    "        # Includes epsilon numerical stabilizer/Tikhonov regularizer.\n",
    "        # Includes running variance update if training and running.\n",
    "        #\n",
    "        if training:\n",
    "            Vrr = xr * xr\n",
    "            Vri = xr * xi\n",
    "            Vii = xi * xi\n",
    "            for d in redux:\n",
    "                Vrr = Vrr.mean(d, keepdim=True)\n",
    "                Vri = Vri.mean(d, keepdim=True)\n",
    "                Vii = Vii.mean(d, keepdim=True)\n",
    "            if self.track_running_stats:\n",
    "                self.RVrr.lerp_(Vrr.squeeze(), exponential_average_factor)\n",
    "                self.RVri.lerp_(Vri.squeeze(), exponential_average_factor)\n",
    "                self.RVii.lerp_(Vii.squeeze(), exponential_average_factor)\n",
    "        else:\n",
    "            Vrr = self.RVrr.view(vdim)\n",
    "            Vri = self.RVri.view(vdim)\n",
    "            Vii = self.RVii.view(vdim)\n",
    "        Vrr = Vrr + self.eps\n",
    "        Vri = Vri\n",
    "        Vii = Vii + self.eps\n",
    "\n",
    "        #\n",
    "        # Matrix Inverse Square Root U = V^-0.5\n",
    "        #\n",
    "        # sqrt of a 2x2 matrix,\n",
    "        # - https://en.wikipedia.org/wiki/Square_root_of_a_2_by_2_matrix\n",
    "        tau = Vrr + Vii\n",
    "        delta = torch.addcmul(Vrr * Vii, -1, Vri, Vri)\n",
    "        s = delta.sqrt()\n",
    "        t = (tau + 2 * s).sqrt()\n",
    "\n",
    "        # matrix inverse, http://mathworld.wolfram.com/MatrixInverse.html\n",
    "        rst = (s * t).reciprocal()\n",
    "        Urr = (s + Vii) * rst\n",
    "        Uii = (s + Vrr) * rst\n",
    "        Uri = (- Vri) * rst\n",
    "\n",
    "        #\n",
    "        # Optionally left-multiply U by affine weights W to produce combined\n",
    "        # weights Z, left-multiply the inputs by Z, then optionally bias them.\n",
    "        #\n",
    "        # y = Zx + B\n",
    "        # y = WUx + B\n",
    "        # y = [Wrr Wri][Urr Uri] [xr] + [Br]\n",
    "        #     [Wir Wii][Uir Uii] [xi]   [Bi]\n",
    "        #\n",
    "        if self.affine:\n",
    "            Wrr, Wri, Wii = self.Wrr.view(vdim), self.Wri.view(vdim), self.Wii.view(vdim)\n",
    "            Zrr = (Wrr * Urr) + (Wri * Uri)\n",
    "            Zri = (Wrr * Uri) + (Wri * Uii)\n",
    "            Zir = (Wri * Urr) + (Wii * Uri)\n",
    "            Zii = (Wri * Uri) + (Wii * Uii)\n",
    "        else:\n",
    "            Zrr, Zri, Zir, Zii = Urr, Uri, Uri, Uii\n",
    "\n",
    "        yr = (Zrr * xr) + (Zri * xi)\n",
    "        yi = (Zir * xr) + (Zii * xi)\n",
    "\n",
    "        if self.affine:\n",
    "            yr = yr + self.Br.view(vdim)\n",
    "            yi = yi + self.Bi.view(vdim)\n",
    "\n",
    "        outputs = torch.cat([yr, yi], self.complex_axis)\n",
    "        return outputs\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
    "               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def complex_cat(inputs, axis):\n",
    "    real, imag = [], []\n",
    "    for idx, data in enumerate(inputs):\n",
    "        r, i = torch.chunk(data, 2, axis)\n",
    "        real.append(r)\n",
    "        imag.append(i)\n",
    "    real = torch.cat(real, axis)\n",
    "    imag = torch.cat(imag, axis)\n",
    "    outputs = torch.cat([real, imag], axis)\n",
    "    return outputs\n",
    "\n",
    "############################################################################\n",
    "#                           for FullSubNet                                 #\n",
    "############################################################################\n",
    "# Source: https://github.com/haoxiangsnr/FullSubNet\n",
    "# from https://github.com/haoxiangsnr/FullSubNet/blob/main/audio_zen/model/module/sequence_model.py\n",
    "# from https://github.com/haoxiangsnr/FullSubNet/blob/main/audio_zen/model/base_model.py\n",
    "# from https://github.com/haoxiangsnr/FullSubNet/blob/main/audio_zen/acoustics/feature.py\n",
    "def stft(y, n_fft=fft_len, hop_length=int(win_len*ola_ratio), win_length=win_len):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: [B, F, T]\n",
    "        n_fft: num of FFT\n",
    "        hop_length: hop length\n",
    "        win_length: window length\n",
    "\n",
    "    Returns:\n",
    "        [B, F, T], **complex-valued** STFT coefficients\n",
    "\n",
    "    \"\"\"\n",
    "    assert y.dim() == 2\n",
    "    return torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        window=torch.hann_window(win_length).to(y.device),\n",
    "        return_complex=True\n",
    "    )\n",
    "\n",
    "\n",
    "def istft(features, n_fft=fft_len, hop_length=int(win_len*ola_ratio), win_length=win_len, length=None, use_mag_phase=False):\n",
    "    \"\"\"\n",
    "    Wrapper for the official torch.istft\n",
    "\n",
    "    Args:\n",
    "        features: [B, F, T, 2] (complex) or ([B, F, T], [B, F, T]) (mag and phase)\n",
    "        n_fft:\n",
    "        hop_length:\n",
    "        win_length:\n",
    "        device:\n",
    "        length:\n",
    "        use_mag_phase: use mag and phase as inputs of iSTFT\n",
    "\n",
    "    Returns:\n",
    "        [B, T]\n",
    "    \"\"\"\n",
    "    if use_mag_phase:\n",
    "        # (mag, phase) or [mag, phase]\n",
    "        assert isinstance(features, tuple) or isinstance(features, list)\n",
    "        mag, phase = features\n",
    "        features = torch.stack([mag * torch.cos(phase), mag * torch.sin(phase)], dim=-1)\n",
    "\n",
    "    return torch.istft(\n",
    "        features,\n",
    "        n_fft,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        window=torch.hann_window(win_length).to(features.device),\n",
    "        length=length\n",
    "    )\n",
    "\n",
    "\n",
    "def mag_phase(complex_tensor):\n",
    "    return torch.abs(complex_tensor), torch.angle(complex_tensor)\n",
    "\n",
    "\n",
    "def build_complex_ideal_ratio_mask(noisy: torch.complex64, clean: torch.complex64) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        noisy: [B, F, T], noisy complex-valued stft coefficients\n",
    "        clean: [B, F, T], clean complex-valued stft coefficients\n",
    "\n",
    "    Returns:\n",
    "        [B, F, T, 2]\n",
    "    \"\"\"\n",
    "    denominator = torch.square(noisy.real) + torch.square(noisy.imag) + EPSILON\n",
    "\n",
    "    mask_real = (noisy.real * clean.real + noisy.imag * clean.imag) / denominator\n",
    "    mask_imag = (noisy.real * clean.imag - noisy.imag * clean.real) / denominator\n",
    "\n",
    "    complex_ratio_mask = torch.stack((mask_real, mask_imag), dim=-1)\n",
    "\n",
    "    return compress_cIRM(complex_ratio_mask, K=10, C=0.1)\n",
    "\n",
    "\n",
    "def compress_cIRM(mask, K=10, C=0.1):\n",
    "    \"\"\"\n",
    "        Compress from (-inf, +inf) to [-K ~ K]\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(mask):\n",
    "        mask = -100 * (mask <= -100) + mask * (mask > -100)\n",
    "        mask = K * (1 - torch.exp(-C * mask)) / (1 + torch.exp(-C * mask))\n",
    "    else:\n",
    "        mask = -100 * (mask <= -100) + mask * (mask > -100)\n",
    "        mask = K * (1 - np.exp(-C * mask)) / (1 + np.exp(-C * mask))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def decompress_cIRM(mask, K=10, limit=9.9):\n",
    "    mask = limit * (mask >= limit) - limit * (mask <= -limit) + mask * (torch.abs(mask) < limit)\n",
    "    mask = -K * torch.log((K - mask) / (K + mask))\n",
    "    return mask\n",
    "\n",
    "\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            output_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            bidirectional,\n",
    "            sequence_model=\"GRU\",\n",
    "            output_activate_function=\"Tanh\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Sequence layer\n",
    "        if sequence_model == \"LSTM\":\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=0.8,\n",
    "            )\n",
    "        elif sequence_model == \"GRU\":\n",
    "            self.sequence_model = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=0.8,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not implemented {sequence_model}\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        if bidirectional:\n",
    "            self.fc_output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "        else:\n",
    "            self.fc_output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Activation function layer\n",
    "        if output_activate_function:\n",
    "            if output_activate_function == \"Tanh\":\n",
    "                self.activate_function = nn.Tanh()\n",
    "            elif output_activate_function == \"ReLU\":\n",
    "                self.activate_function = nn.ReLU()\n",
    "            elif output_activate_function == \"ReLU6\":\n",
    "                self.activate_function = nn.ReLU6()\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Not implemented activation function {self.activate_function}\")\n",
    "\n",
    "        self.output_activate_function = output_activate_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, F, T]\n",
    "        Returns:\n",
    "            [B, F, T]\n",
    "        \"\"\"\n",
    "        assert x.dim() == 3\n",
    "        self.sequence_model.flatten_parameters()\n",
    "\n",
    "        x = x.permute(0, 2, 1).contiguous()  # [B, F, T] => [B, T, F]\n",
    "        o, _ = self.sequence_model(x)\n",
    "        o = self.fc_output_layer(o)\n",
    "        if self.output_activate_function:\n",
    "            o = self.activate_function(o)\n",
    "        o = o.permute(0, 2, 1).contiguous()  # [B, T, F] => [B, F, T]\n",
    "        return o\n",
    "    \n",
    "    \n",
    "EPSILON = np.finfo(np.float32).eps\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def unfold(input, num_neighbor):\n",
    "        \"\"\"\n",
    "        Along with the frequency dim, split overlapped sub band units from spectrogram.\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "            num_neighbor:\n",
    "\n",
    "        Returns:\n",
    "            [B, N, C, F_s, T], F, e.g. [2, 161, 1, 19, 200]\n",
    "        \"\"\"\n",
    "        assert input.dim() == 4, f\"The dim of input is {input.dim()}. It should be four dim.\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = input.size()\n",
    "\n",
    "        if num_neighbor < 1:\n",
    "            # No change for the input\n",
    "            return input.permute(0, 2, 1, 3).reshape(batch_size, num_freqs, num_channels, 1, num_frames)\n",
    "\n",
    "        output = input.reshape(batch_size * num_channels, 1, num_freqs, num_frames)\n",
    "        sub_band_unit_size = num_neighbor * 2 + 1\n",
    "\n",
    "        # Pad to the top and bottom\n",
    "        output = F.pad(output, [0, 0, num_neighbor, num_neighbor], mode=\"reflect\")\n",
    "\n",
    "        output = F.unfold(output, (sub_band_unit_size, num_frames))\n",
    "        assert output.shape[-1] == num_freqs, f\"n_freqs != N (sub_band), {num_freqs} != {output.shape[-1]}\"\n",
    "\n",
    "        # Split the dim of the unfolded feature\n",
    "        output = output.reshape(batch_size, num_channels, sub_band_unit_size, num_frames, num_freqs)\n",
    "        output = output.permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _reduce_complexity_separately(sub_band_input, full_band_output, device):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            sub_band_input: [60, 257, 1, 33, 200]\n",
    "            full_band_output: [60, 257, 1, 3, 200]\n",
    "            device:\n",
    "\n",
    "        Notes:\n",
    "            1. 255 and 256 freq not able to be trained\n",
    "            2. batch size \n",
    "\n",
    "        Returns:\n",
    "            [60, 85, 1, 36, 200]\n",
    "        \"\"\"\n",
    "        batch_size = full_band_output.shape[0]\n",
    "        n_freqs = full_band_output.shape[1]\n",
    "        sub_batch_size = batch_size // 3\n",
    "        final_selected = []\n",
    "\n",
    "        for idx in range(3):\n",
    "            # [0, 60) => [0, 20)\n",
    "            sub_batch_indices = torch.arange(idx * sub_batch_size, (idx + 1) * sub_batch_size, device=device)\n",
    "            full_band_output_sub_batch = torch.index_select(full_band_output, dim=0, index=sub_batch_indices)\n",
    "            sub_band_output_sub_batch = torch.index_select(sub_band_input, dim=0, index=sub_batch_indices)\n",
    "\n",
    "            # Avoid to use padded value (first freq and last freq)\n",
    "            # i = 0, (1, 256, 3) = [1, 4, ..., 253]\n",
    "            # i = 1, (2, 256, 3) = [2, 5, ..., 254]\n",
    "            # i = 2, (3, 256, 3) = [3, 6, ..., 255]\n",
    "            freq_indices = torch.arange(idx + 1, n_freqs - 1, step=3, device=device)\n",
    "            full_band_output_sub_batch = torch.index_select(full_band_output_sub_batch, dim=1, index=freq_indices)\n",
    "            sub_band_output_sub_batch = torch.index_select(sub_band_output_sub_batch, dim=1, index=freq_indices)\n",
    "\n",
    "            # ([30, 85, 1, 33 200], [30, 85, 1, 3, 200]) => [30, 85, 1, 36, 200]\n",
    "\n",
    "            final_selected.append(torch.cat([sub_band_output_sub_batch, full_band_output_sub_batch], dim=-2))\n",
    "\n",
    "        return torch.cat(final_selected, dim=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def sband_forgetting_norm(input, train_sample_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input:\n",
    "            train_sample_length:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        assert input.ndim == 3\n",
    "        batch_size, n_freqs, n_frames = input.size()\n",
    "\n",
    "        eps = 1e-10\n",
    "        alpha = (train_sample_length - 1) / (train_sample_length + 1)\n",
    "        mu = 0\n",
    "        mu_list = []\n",
    "\n",
    "        for idx in range(input.shape[-1]):\n",
    "            if idx < train_sample_length:\n",
    "                alp = torch.min(torch.tensor([(idx - 1) / (idx + 1), alpha]))\n",
    "                mu = alp * mu + (1 - alp) * torch.mean(input[:, :, idx], dim=1).reshape(batch_size, 1)  # [B, 1]\n",
    "            else:\n",
    "                mu = alpha * mu + (1 - alpha) * input[:, (n_freqs // 2 - 1), idx].reshape(batch_size, 1)\n",
    "\n",
    "            mu_list.append(mu)\n",
    "\n",
    "            # print(\"input\", input[:, :, idx].min(), input[:, :, idx].max(), input[:, :, idx].mean())\n",
    "            # print(f\"alp {idx}: \", alp)\n",
    "            # print(f\"mu {idx}: {mu[128, 0]}\")\n",
    "\n",
    "        mu = torch.stack(mu_list, dim=-1)  # [B, 1, T]\n",
    "        input = input / (mu + eps)\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def forgetting_norm(input, sample_length_in_training):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: [B, F, T]\n",
    "            sample_length_in_training:\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        assert input.ndim == 3\n",
    "        batch_size, n_freqs, n_frames = input.size()\n",
    "        eps = 1e-10\n",
    "        mu = 0\n",
    "        alpha = (sample_length_in_training - 1) / (sample_length_in_training + 1)\n",
    "\n",
    "        mu_list = []\n",
    "        for idx in range(input.shape[-1]):\n",
    "            if idx < sample_length_in_training:\n",
    "                alp = torch.min(torch.tensor([(idx - 1) / (idx + 1), alpha]))\n",
    "                mu = alp * mu + (1 - alp) * torch.mean(input[:, :, idx], dim=1).reshape(batch_size, 1)  # [B, 1]\n",
    "            else:\n",
    "                current_frame_mu = torch.mean(input[:, :, idx], dim=1).reshape(batch_size, 1)  # [B, 1]\n",
    "                mu = alpha * mu + (1 - alpha) * current_frame_mu\n",
    "\n",
    "            mu_list.append(mu)\n",
    "\n",
    "            # print(\"input\", input[:, :, idx].min(), input[:, :, idx].max(), input[:, :, idx].mean())\n",
    "            # print(f\"alp {idx}: \", alp)\n",
    "            # print(f\"mu {idx}: {mu[128, 0]}\")\n",
    "\n",
    "        mu = torch.stack(mu_list, dim=-1)  # [B, 1, T]\n",
    "        input = input / (mu + eps)\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def hybrid_norm(input, sample_length_in_training=192):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: [B, F, T]\n",
    "            sample_length_in_training:\n",
    "\n",
    "        Returns:\n",
    "            [B, F, T]\n",
    "        \"\"\"\n",
    "        assert input.ndim == 3\n",
    "        device = input.device\n",
    "        data_type = input.dtype\n",
    "        batch_size, n_freqs, n_frames = input.size()\n",
    "        eps = 1e-10\n",
    "\n",
    "        mu = 0\n",
    "        alpha = (sample_length_in_training - 1) / (sample_length_in_training + 1)\n",
    "        mu_list = []\n",
    "        for idx in range(input.shape[-1]):\n",
    "            if idx < sample_length_in_training:\n",
    "                alp = torch.min(torch.tensor([(idx - 1) / (idx + 1), alpha]))\n",
    "                mu = alp * mu + (1 - alp) * torch.mean(input[:, :, idx], dim=1).reshape(batch_size, 1)  # [B, 1]\n",
    "                mu_list.append(mu)\n",
    "            else:\n",
    "                break\n",
    "        initial_mu = torch.stack(mu_list, dim=-1)  # [B, 1, T]\n",
    "\n",
    "        step_sum = torch.sum(input, dim=1)  # [B, T]\n",
    "        cumulative_sum = torch.cumsum(step_sum, dim=-1)  # [B, T]\n",
    "\n",
    "        entry_count = torch.arange(n_freqs, n_freqs * n_frames + 1, n_freqs, dtype=data_type, device=device)\n",
    "        entry_count = entry_count.reshape(1, n_frames)  # [1, T]\n",
    "        entry_count = entry_count.expand_as(cumulative_sum)  # [1, T] => [B, T]\n",
    "\n",
    "        cum_mean = cumulative_sum / entry_count  # B, T\n",
    "\n",
    "        cum_mean = cum_mean.reshape(batch_size, 1, n_frames)  # [B, 1, T]\n",
    "\n",
    "        # print(initial_mu[0, 0, :50])\n",
    "        # print(\"-\"*60)\n",
    "        # print(cum_mean[0, 0, :50])\n",
    "        cum_mean[:, :, :sample_length_in_training] = initial_mu\n",
    "\n",
    "        return input / (cum_mean + eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def offline_laplace_norm(input):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        # utterance-level mu\n",
    "        mu = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "        normed = input / (mu + 1e-5)\n",
    "\n",
    "        return normed\n",
    "\n",
    "    @staticmethod\n",
    "    def cumulative_laplace_norm(input):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = input.size()\n",
    "        input = input.reshape(batch_size * num_channels, num_freqs, num_frames)\n",
    "\n",
    "        step_sum = torch.sum(input, dim=1)  # [B * C, F, T] => [B, T]\n",
    "        cumulative_sum = torch.cumsum(step_sum, dim=-1)  # [B, T]\n",
    "\n",
    "        entry_count = torch.arange(\n",
    "            num_freqs,\n",
    "            num_freqs * num_frames + 1,\n",
    "            num_freqs,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device\n",
    "        )\n",
    "        entry_count = entry_count.reshape(1, num_frames)  # [1, T]\n",
    "        entry_count = entry_count.expand_as(cumulative_sum)  # [1, T] => [B, T]\n",
    "\n",
    "        cumulative_mean = cumulative_sum / entry_count  # B, T\n",
    "        cumulative_mean = cumulative_mean.reshape(batch_size * num_channels, 1, num_frames)\n",
    "\n",
    "        normed = input / (cumulative_mean + EPSILON)\n",
    "\n",
    "        return normed.reshape(batch_size, num_channels, num_freqs, num_frames)\n",
    "\n",
    "    @staticmethod\n",
    "    def offline_gaussian_norm(input):\n",
    "        \"\"\"\n",
    "        Zero-Norm\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        mu = torch.mean(input, dim=(1, 2, 3), keepdim=True)\n",
    "        std = torch.std(input, dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "        normed = (input - mu) / (std + 1e-5)\n",
    "\n",
    "        return normed\n",
    "\n",
    "    @staticmethod\n",
    "    def cumulative_layer_norm(input):\n",
    "        \"\"\"\n",
    "        Online zero-norm\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = input.size()\n",
    "        input = input.reshape(batch_size * num_channels, num_freqs, num_frames)\n",
    "\n",
    "        step_sum = torch.sum(input, dim=1)  # [B * C, F, T] => [B, T]\n",
    "        step_pow_sum = torch.sum(torch.square(input), dim=1)\n",
    "\n",
    "        cumulative_sum = torch.cumsum(step_sum, dim=-1)  # [B, T]\n",
    "        cumulative_pow_sum = torch.cumsum(step_pow_sum, dim=-1)  # [B, T]\n",
    "\n",
    "        entry_count = torch.arange(\n",
    "            num_freqs,\n",
    "            num_freqs * num_frames + 1,\n",
    "            num_freqs,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device\n",
    "        )\n",
    "        entry_count = entry_count.reshape(1, num_frames)  # [1, T]\n",
    "        entry_count = entry_count.expand_as(cumulative_sum)  # [1, T] => [B, T]\n",
    "\n",
    "        cumulative_mean = cumulative_sum / entry_count  # [B, T]\n",
    "        cumulative_var = (\n",
    "                                 cumulative_pow_sum - 2 * cumulative_mean * cumulative_sum) / entry_count + cumulative_mean.pow(\n",
    "            2)  # [B, T]\n",
    "        cumulative_std = torch.sqrt(cumulative_var + EPSILON)  # [B, T]\n",
    "\n",
    "        cumulative_mean = cumulative_mean.reshape(batch_size * num_channels, 1, num_frames)\n",
    "        cumulative_std = cumulative_std.reshape(batch_size * num_channels, 1, num_frames)\n",
    "\n",
    "        normed = (input - cumulative_mean) / cumulative_std\n",
    "\n",
    "        return normed.reshape(batch_size, num_channels, num_freqs, num_frames)\n",
    "\n",
    "    def norm_wrapper(self, norm_type: str):\n",
    "        if norm_type == \"offline_laplace_norm\":\n",
    "            norm = self.offline_laplace_norm\n",
    "        elif norm_type == \"cumulative_laplace_norm\":\n",
    "            norm = self.cumulative_laplace_norm\n",
    "        elif norm_type == \"offline_gaussian_norm\":\n",
    "            norm = self.offline_gaussian_norm\n",
    "        elif norm_type == \"cumulative_layer_norm\":\n",
    "            norm = self.cumulative_layer_norm\n",
    "        else:\n",
    "            raise NotImplementedError(\"You must set up a type of Norm. \"\n",
    "                                      \"e.g. offline_laplace_norm, cumulative_laplace_norm, forgetting_norm, etc.\")\n",
    "        return norm\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        \"\"\"\n",
    "        Usage:\n",
    "            model = Model()\n",
    "            model.apply(weight_init)\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            init.normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.Conv3d):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.ConvTranspose1d):\n",
    "            init.normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.ConvTranspose3d):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "            init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "            init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.BatchNorm3d):\n",
    "            init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "            init.constant_(m.bias.data, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "        elif isinstance(m, nn.LSTMCell):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "        elif isinstance(m, nn.GRUCell):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                         for data normalization                           #\n",
    "############################################################################\n",
    "# get mu and sig\n",
    "def get_mu_sig(data):\n",
    "    \"\"\"Compute mean and standard deviation vector of input data\n",
    "\n",
    "    Returns:\n",
    "        mu: mean vector (#dim by one)\n",
    "        sig: standard deviation vector (#dim by one)\n",
    "    \"\"\"\n",
    "    # Initialize array.\n",
    "    data_num = len(data)\n",
    "    mu_utt = []\n",
    "    tmp_utt = []\n",
    "    for n in range(data_num):\n",
    "        dim = len(data[n])\n",
    "        mu_utt_tmp = np.zeros(dim)\n",
    "        mu_utt.append(mu_utt_tmp)\n",
    "\n",
    "        tmp_utt_tmp = np.zeros(dim)\n",
    "        tmp_utt.append(tmp_utt_tmp)\n",
    "\n",
    "    # Get mean.\n",
    "    for n in range(data_num):\n",
    "        mu_utt[n] = np.mean(data[n], 0)\n",
    "    mu = mu_utt\n",
    "\n",
    "    # Get standard deviation.\n",
    "    for n in range(data_num):\n",
    "        tmp_utt[n] = np.mean(np.square(data[n] - mu[n]), 0)\n",
    "    sig = np.sqrt(tmp_utt)\n",
    "\n",
    "    # Assign unit variance.\n",
    "    for n in range(len(sig)):\n",
    "        if sig[n] < 1e-5:\n",
    "            sig[n] = 1.0\n",
    "    return np.float16(mu), np.float16(sig)\n",
    "\n",
    "\n",
    "def get_statistics_inp(inp):\n",
    "    \"\"\"Get statistical parameter of input data.\n",
    "\n",
    "    Args:\n",
    "        inp: input data\n",
    "\n",
    "    Returns:\n",
    "        mu_inp: mean vector of input data\n",
    "        sig_inp: standard deviation vector of input data\n",
    "    \"\"\"\n",
    "\n",
    "    mu_inp, sig_inp = get_mu_sig(inp)\n",
    "\n",
    "    return mu_inp, sig_inp\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                       for plotting the samples                           #\n",
    "############################################################################\n",
    "def hann_window(win_samp):\n",
    "    tmp = np.arange(1, win_samp + 1, 1.0, dtype=np.float64)\n",
    "    window = 0.5 - 0.5 * np.cos((2.0 * np.pi * tmp) / (win_samp + 1))\n",
    "    return np.float32(window)\n",
    "\n",
    "\n",
    "def fig2np(fig):\n",
    "    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_spectrogram_to_numpy(input_wav, fs, n_fft, n_overlap, mode, clim, label):\n",
    "    # cuda to cpu\n",
    "    input_wav = input_wav.cpu().detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "    if mode == 'phase':\n",
    "        pxx, freq, t, cax = plt.specgram(input_wav, NFFT=int(n_fft), Fs=int(fs), noverlap=n_overlap,\n",
    "                                         cmap='jet',\n",
    "                                         mode=mode)\n",
    "    else:\n",
    "        pxx, freq, t, cax = plt.specgram(input_wav, NFFT=int(n_fft), Fs=int(fs), noverlap=n_overlap,\n",
    "                                         cmap='jet')\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.tight_layout()\n",
    "    plt.clim(clim)\n",
    "\n",
    "    if label is None:\n",
    "        fig.colorbar(cax)\n",
    "    else:\n",
    "        fig.colorbar(cax, label=label)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    data = fig2np(fig)\n",
    "    plt.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_mask_to_numpy(mask, fs, n_fft, n_overlap, clim1, clim2, cmap):\n",
    "    frame_num = mask.shape[0]\n",
    "    shift_length = n_overlap\n",
    "    frame_length = n_fft\n",
    "    signal_length = frame_num * shift_length + frame_length\n",
    "\n",
    "    xt = np.arange(0, np.floor(10 * signal_length / fs) / 10, step=0.5) / (signal_length / fs) * frame_num + 1e-8\n",
    "    yt = (n_fft / 2) / (fs / 1000 / 2) * np.arange(0, (fs / 1000 / 2) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    im = ax.imshow(np.transpose(mask), aspect='auto', origin='lower', interpolation='none', cmap=cmap)\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (kHz)')\n",
    "    plt.xticks(xt, np.arange(0, np.floor(10 * (signal_length / fs)) / 10, step=0.5))\n",
    "    plt.yticks(yt, np.int16(np.linspace(0, int((fs / 1000) / 2), len(yt))))\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    im.set_clim(clim1, clim2)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    data = fig2np(fig)\n",
    "    plt.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def plot_error_to_numpy(estimated, target, fs, n_fft, n_overlap, mode, clim1, clim2, label):\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    if mode is None:\n",
    "        pxx1, freq, t, cax = plt.specgram(estimated, NFFT=n_fft, Fs=int(fs), noverlap=n_overlap, cmap='jet')\n",
    "        pxx2, freq, t, cax = plt.specgram(target, NFFT=n_fft, Fs=int(fs), noverlap=n_overlap, cmap='jet')\n",
    "        im = ax.imshow(10 * np.log10(pxx1) - 10 * np.log10(pxx2), aspect='auto', origin='lower', interpolation='none',\n",
    "                       cmap='jet')\n",
    "    else:\n",
    "        pxx1, freq, t, cax = plt.specgram(estimated, NFFT=n_fft, Fs=int(fs), noverlap=n_overlap, cmap='jet',\n",
    "                                          mode=mode)\n",
    "        pxx2, freq, t, cax = plt.specgram(target, NFFT=n_fft, Fs=int(fs), noverlap=n_overlap, cmap='jet',\n",
    "                                          mode=mode)\n",
    "        im = ax.imshow(pxx1 - pxx2, aspect='auto', origin='lower', interpolation='none', cmap='jet')\n",
    "\n",
    "    frame_num = pxx1.shape[1]\n",
    "    shift_length = n_overlap\n",
    "    frame_length = n_fft\n",
    "    signal_length = frame_num * shift_length + frame_length\n",
    "\n",
    "    xt = np.arange(0, np.floor(10 * (signal_length / fs)) / 10, step=0.5) / (signal_length / fs) * frame_num\n",
    "    yt = (n_fft / 2) / (fs / 1000 / 2) * np.arange(0, (fs / 1000 / 2) + 1)\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (kHz)')\n",
    "    plt.xticks(xt, np.arange(0, np.floor(10 * (signal_length / fs)) / 10, step=0.5))\n",
    "    plt.yticks(yt, np.int16(np.linspace(0, int((fs / 1000) / 2), len(yt))))\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar(im, ax=ax, label=label)\n",
    "    im.set_clim(clim1, clim2)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    data = fig2np(fig)\n",
    "    plt.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                              for trainer.py                              #\n",
    "############################################################################\n",
    "class Bar(object):\n",
    "    def __init__(self, dataloader):\n",
    "        if not hasattr(dataloader, 'dataset'):\n",
    "            raise ValueError('Attribute `dataset` not exists in dataloder.')\n",
    "        if not hasattr(dataloader, 'batch_size'):\n",
    "            raise ValueError('Attribute `batch_size` not exists in dataloder.')\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(dataloader)\n",
    "        self.dataset = dataloader.dataset\n",
    "        self.batch_size = dataloader.batch_size\n",
    "        self._idx = 0\n",
    "        self._batch_idx = 0\n",
    "        self._time = []\n",
    "        self._DISPLAY_LENGTH = 50\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self._time) < 2:\n",
    "            self._time.append(time.time())\n",
    "\n",
    "        self._batch_idx += self.batch_size\n",
    "        if self._batch_idx > len(self.dataset):\n",
    "            self._batch_idx = len(self.dataset)\n",
    "\n",
    "        try:\n",
    "            batch = next(self.iterator)\n",
    "            self._display()\n",
    "        except StopIteration:\n",
    "            raise StopIteration()\n",
    "\n",
    "        self._idx += 1\n",
    "        if self._idx >= len(self.dataloader):\n",
    "            self._reset()\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def _display(self):\n",
    "        if len(self._time) > 1:\n",
    "            t = (self._time[-1] - self._time[-2])\n",
    "            eta = t * (len(self.dataloader) - self._idx)\n",
    "        else:\n",
    "            eta = 0\n",
    "\n",
    "        rate = self._idx / len(self.dataloader)\n",
    "        len_bar = int(rate * self._DISPLAY_LENGTH)\n",
    "        bar = ('=' * len_bar + '>').ljust(self._DISPLAY_LENGTH, '.')\n",
    "        idx = str(self._batch_idx).rjust(len(str(len(self.dataset))), ' ')\n",
    "\n",
    "        tmpl = '\\r{}/{}: [{}] - ETA {:.1f}s'.format(\n",
    "            idx,\n",
    "            len(self.dataset),\n",
    "            bar,\n",
    "            eta\n",
    "        )\n",
    "        print(tmpl, end='')\n",
    "        if self._batch_idx == len(self.dataset):\n",
    "            print()\n",
    "\n",
    "    def _reset(self):\n",
    "        self._idx = 0\n",
    "        self._batch_idx = 0\n",
    "        self._time = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c93bc9-3db2-440e-b686-8ba6f5ce6b3d",
   "metadata": {},
   "source": [
    "### Tools for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314c813e-7738-423a-8bfb-1f11c21f280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory(dir_name):\n",
    "    \n",
    "    if os.path.isdir(dir_name) is False:\n",
    "        print(\"[Error] There is no directory '%s'.\" % dir_name)\n",
    "        exit()\n",
    "    \n",
    "    addrs = []\n",
    "    for subdir, dirs, files in os.walk(dir_name):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                filepath = subdir + file\n",
    "                addrs.append(filepath)\n",
    "    return addrs\n",
    "\n",
    "def find_pair(noisy_dirs):\n",
    "    clean_dirs = []\n",
    "    for i in range(len(noisy_dirs)):\n",
    "        addrs = noisy_dirs[i]\n",
    "        if addrs.endswith(\".wav\"):\n",
    "            addr_noisy = str(addrs)\n",
    "            addr_clean = str(addrs).replace('noisy', 'clean')\n",
    "            clean_dirs.append(addr_clean)\n",
    "    return clean_dirs\n",
    "\n",
    "def addr2wav(addr):\n",
    "    #wav, fs = soundfile.read(addr)\n",
    "    wav, fs = librosa.load(addr,sr=8000)\n",
    "    \n",
    "    print(f\"[addr2wav] wav {wav} fs {fs} addr {addr}\")\n",
    "    # normalize\n",
    "    wav = minMaxNorm(wav)\n",
    "    return wav\n",
    "    #######################################################################\n",
    "\n",
    "#                        Data Normalization                           #\n",
    "#######################################################################\n",
    "def minMaxNorm(wav, eps=1e-8):\n",
    "    max = np.max(abs(wav))\n",
    "    min = np.min(abs(wav))\n",
    "    wav = (wav - min) / (max - min + eps)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def generate_padded_samples(original, source, output_length, sample_rate, types):\n",
    "    \"\"\"\n",
    "\n",
    "    :param original:\n",
    "    :param source:\n",
    "    :param output_length:\n",
    "    :param sample_rate:\n",
    "    :param types:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    copy = np.zeros(output_length, dtype=np.float32)\n",
    "    src_length = len(source)\n",
    "    left = output_length - src_length  # amount to be padded\n",
    "    # pad front or back\n",
    "    prob = random.random()\n",
    "\n",
    "    aug = original\n",
    "\n",
    "    while len(aug) < left:\n",
    "        aug = np.concatenate([aug, aug])\n",
    "\n",
    "    if prob < 0.5:\n",
    "        # pad back\n",
    "        copy[left:] = source\n",
    "        copy[:left] = aug[len(aug) - left:]\n",
    "    else:\n",
    "        # pad front\n",
    "        copy[:src_length] = source[:]\n",
    "        copy[src_length:] = aug[:left]\n",
    "\n",
    "    return copy\n",
    "\n",
    "\n",
    "def split_and_pad(original, desired_length, sample_rate, types=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param original:\n",
    "    :param desired_length:\n",
    "    :param sample_rate:\n",
    "    :param types:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    output_buffer_length = int(desired_length * sample_rate)\n",
    "    sound_clip = original[0].copy()\n",
    "    n_samples = len(sound_clip)\n",
    "\n",
    "    output = []\n",
    "    # if: the audio sample length > desiredLength, then split & pad\n",
    "    # else: simply pad according to given type 1 or 2\n",
    "    if n_samples > output_buffer_length:\n",
    "        frames = librosa.util.frame(sound_clip, frame_length=output_buffer_length, hop_length=output_buffer_length // 2,\n",
    "                                    axis=0)\n",
    "        for i in range(frames.shape[0]):\n",
    "            output.append((frames[i]))\n",
    "\n",
    "        last_id = frames.shape[0] * (output_buffer_length // 2)\n",
    "        last_sample = sound_clip[last_id:];\n",
    "        pad_times = (output_buffer_length - len(last_sample)) / len(last_sample)\n",
    "        padded = generate_padded_samples(sound_clip, last_sample, output_buffer_length, sample_rate, types)\n",
    "        output.append(padded)\n",
    "\n",
    "    else:\n",
    "        padded = generate_padded_samples(sound_clip, sound_clip, output_buffer_length, sample_rate, types);\n",
    "        pad_times = (output_buffer_length - len(sound_clip)) / len(sound_clip)\n",
    "        output.append(padded)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd677e-b7af-44e4-be0b-aea25a7f7330",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9758cc-b90a-4c0c-837d-aac05068a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(mode, type=0, snr=0):\n",
    "    if mode == 'train':\n",
    "        return DataLoader(\n",
    "            dataset=Wave_Dataset(mode, type, snr),\n",
    "            batch_size=batch,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            sampler=None\n",
    "        )\n",
    "    elif mode == 'valid':\n",
    "        return DataLoader(\n",
    "            dataset=Wave_Dataset(mode, type, snr),\n",
    "            batch_size=batch, shuffle=False, num_workers=0\n",
    "        )\n",
    "    elif mode == 'test':\n",
    "        return DataLoader(\n",
    "            dataset=Wave_Dataset(mode, type, snr),\n",
    "            batch_size=batch, shuffle=False, num_workers=0\n",
    "        )\n",
    "\n",
    "\n",
    "class Wave_Dataset(Dataset):\n",
    "    def __init__(self, mode, type, snr):\n",
    "        # load data\n",
    "        if mode == 'train':\n",
    "            self.mode = 'train'\n",
    "            print('<Training dataset>')\n",
    "            print('Load the data...')\n",
    "            self.input_path = HOME_DIR+\"kumc_lung_clinical_study/train.npy\"\n",
    "            self.input = np.load(self.input_path)\n",
    "        elif mode == 'valid':\n",
    "            self.mode = 'valid'\n",
    "            print('<Validation dataset>')\n",
    "            print('Load the data...')\n",
    "            self.input_path =  HOME_DIR+\"kumc_lung_clinical_study/val.npy\"\n",
    "            self.input = np.load(self.input_path)\n",
    "            # # if you want to use a part of the dataset\n",
    "            # self.input = self.input[:500]\n",
    "        elif mode == 'test':\n",
    "            self.mode = 'test'\n",
    "            print('<Test dataset>')\n",
    "            print('Load the data...')\n",
    "            self.input_path = HOME_DIR+\"kumc_lung_clinical_study/test.npy\"\n",
    "\n",
    "            self.input = np.load(self.input_path)\n",
    "            #self.input = self.input[type][snr]\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inputs = self.input[idx][0]\n",
    "        targets = self.input[idx][1]\n",
    "\n",
    "        # transform to torch from numpy\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        targets = torch.from_numpy(targets)\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f549656-fcbd-49e3-97f1-e9a70ebea661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveFile_Dataset(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        # load data\n",
    "        if mode == 'train':\n",
    "            print('<Training dataset>')\n",
    "            print('Load the data...')\n",
    "            # load the wav addr\n",
    "            self.noisy_dirs = scan_directory(noisy_dirs_for_train)\n",
    "            self.clean_dirs = find_pair(self.noisy_dirs)\n",
    "\n",
    "        elif mode == 'valid':\n",
    "            print('<Validation dataset>')\n",
    "            print('Load the data...')\n",
    "            # load the wav addr\n",
    "            self.noisy_dirs = scan_directory(noisy_dirs_for_valid)\n",
    "            self.clean_dirs = find_pair(self.noisy_dirs)\n",
    "            \n",
    "        elif mode == 'test':\n",
    "            print('<Validation dataset>')\n",
    "            print('Load the data...')\n",
    "            # load the wav addr\n",
    "            self.noisy_dirs = scan_directory(HOME_DIR+\"kumc_lung_clinical_study/eval/\")\n",
    "            print(f\"self.noisy_dirs {self.noisy_dirs}\")\n",
    "            self.clean_dirs = self.noisy_dirs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read the wav\n",
    "        inputs = addr2wav(self.noisy_dirs[idx])\n",
    "        targets = addr2wav(self.clean_dirs[idx])\n",
    "        \n",
    "        print(inputs)\n",
    "        print(targets)\n",
    "\n",
    "        # transform to torch from numpy\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        targets = torch.from_numpy(targets)\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05088c-d787-4366-aae0-db9db880f1de",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49006539-5a24-47fa-8af4-ac04fb66007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCCRN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            rnn_layers = rnn_layers,\n",
    "            rnn_units = rnn_units,\n",
    "            win_len = win_len,\n",
    "            win_inc = win_inc,\n",
    "            fft_len = fft_len,\n",
    "            win_type =window,\n",
    "            masking_mode = masking_mode,\n",
    "            use_cbn=False,\n",
    "            kernel_size=5\n",
    "    ):\n",
    "        '''\n",
    "            rnn_layers: the number of lstm layers in the crn,\n",
    "            rnn_units: for clstm, rnn_units = real+imag\n",
    "        '''\n",
    "\n",
    "        super(DCCRN, self).__init__()\n",
    "\n",
    "        # for fft\n",
    "        self.win_len = win_len\n",
    "        self.win_inc = win_inc\n",
    "        self.fft_len = fft_len\n",
    "        self.win_type = win_type\n",
    "\n",
    "        input_dim = win_len\n",
    "        output_dim = win_len\n",
    "\n",
    "        self.rnn_units = rnn_units\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layers = rnn_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        kernel_num = dccrn_kernel_num\n",
    "        self.kernel_num = [2] + kernel_num\n",
    "        self.masking_mode = masking_mode\n",
    "\n",
    "        # bidirectional=True\n",
    "        bidirectional = False\n",
    "        fac = 2 if bidirectional else 1\n",
    "\n",
    "        fix = True\n",
    "        self.fix = fix\n",
    "        self.stft = ConvSTFT(self.win_len, self.win_inc, fft_len, self.win_type, 'complex', fix=fix)\n",
    "        self.istft = ConviSTFT(self.win_len, self.win_inc, fft_len, self.win_type, 'complex', fix=fix)\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for idx in range(len(self.kernel_num) - 1):\n",
    "            self.encoder.append(\n",
    "                nn.Sequential(\n",
    "                    # nn.ConstantPad2d([0, 0, 0, 0], 0),\n",
    "                    ComplexConv2d(\n",
    "                        self.kernel_num[idx],\n",
    "                        self.kernel_num[idx + 1],\n",
    "                        kernel_size=(self.kernel_size, 2),\n",
    "                        stride=(2, 1),\n",
    "                        padding=(2, 1)\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(self.kernel_num[idx + 1]) if not use_cbn else ComplexBatchNorm(\n",
    "                        self.kernel_num[idx + 1]),\n",
    "                    nn.PReLU()\n",
    "                )\n",
    "            )\n",
    "        hidden_dim = self.fft_len // (2 ** (len(self.kernel_num)))\n",
    "\n",
    "        if lstm == 'complex':\n",
    "            rnns = []\n",
    "            for idx in range(rnn_layers):\n",
    "                rnns.append(\n",
    "                    NavieComplexLSTM(\n",
    "                        input_size=hidden_dim * self.kernel_num[-1] if idx == 0 else self.rnn_units,\n",
    "                        hidden_size=self.rnn_units,\n",
    "                        bidirectional=bidirectional,\n",
    "                        batch_first=False,\n",
    "                        projection_dim=hidden_dim * self.kernel_num[-1] if idx == rnn_layers - 1 else None,\n",
    "                    )\n",
    "                )\n",
    "                self.enhance = nn.Sequential(*rnns)\n",
    "        else:\n",
    "            self.enhance = nn.LSTM(\n",
    "                input_size=hidden_dim * self.kernel_num[-1],\n",
    "                hidden_size=self.rnn_units,\n",
    "                num_layers=2,\n",
    "                dropout=0.0,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=False\n",
    "            )\n",
    "            self.tranform = nn.Linear(self.rnn_units * fac, hidden_dim * self.kernel_num[-1])\n",
    "\n",
    "        if skip_type:\n",
    "            for idx in range(len(self.kernel_num) - 1, 0, -1):\n",
    "                if idx != 1:\n",
    "                    self.decoder.append(\n",
    "                        nn.Sequential(\n",
    "                            ComplexConvTranspose2d(\n",
    "                                self.kernel_num[idx] * 2,\n",
    "                                self.kernel_num[idx - 1],\n",
    "                                kernel_size=(self.kernel_size, 2),\n",
    "                                stride=(2, 1),\n",
    "                                padding=(2, 0),\n",
    "                                output_padding=(1, 0)\n",
    "                            ),\n",
    "                            nn.BatchNorm2d(self.kernel_num[idx - 1]) if not use_cbn else ComplexBatchNorm(\n",
    "                                self.kernel_num[idx - 1]),\n",
    "                            nn.PReLU()\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    self.decoder.append(\n",
    "                        nn.Sequential(\n",
    "                            ComplexConvTranspose2d(\n",
    "                                self.kernel_num[idx] * 2,\n",
    "                                self.kernel_num[idx - 1],\n",
    "                                kernel_size=(self.kernel_size, 2),\n",
    "                                stride=(2, 1),\n",
    "                                padding=(2, 0),\n",
    "                                output_padding=(1, 0)\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "        else:  # you can erase the skip connection\n",
    "            for idx in range(len(self.kernel_num) - 1, 0, -1):\n",
    "                if idx != 1:\n",
    "                    self.decoder.append(\n",
    "                        nn.Sequential(\n",
    "                            ComplexConvTranspose2d(\n",
    "                                self.kernel_num[idx],\n",
    "                                self.kernel_num[idx - 1],\n",
    "                                kernel_size=(self.kernel_size, 2),\n",
    "                                stride=(2, 1),\n",
    "                                padding=(2, 0),\n",
    "                                output_padding=(1, 0)\n",
    "                            ),\n",
    "                            nn.BatchNorm2d(self.kernel_num[idx - 1]) if not use_cbn else ComplexBatchNorm(\n",
    "                                self.kernel_num[idx - 1]),\n",
    "                            # nn.ELU()\n",
    "                            nn.PReLU()\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    self.decoder.append(\n",
    "                        nn.Sequential(\n",
    "                            ComplexConvTranspose2d(\n",
    "                                self.kernel_num[idx],\n",
    "                                self.kernel_num[idx - 1],\n",
    "                                kernel_size=(self.kernel_size, 2),\n",
    "                                stride=(2, 1),\n",
    "                                padding=(2, 0),\n",
    "                                output_padding=(1, 0)\n",
    "                            ),\n",
    "                        )\n",
    "                    )\n",
    "        self.flatten_parameters()\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        if isinstance(self.enhance, nn.LSTM):\n",
    "            self.enhance.flatten_parameters()\n",
    "\n",
    "    def forward(self, inputs, targets=0):\n",
    "        specs = self.stft(inputs)\n",
    "        real = specs[:, :self.fft_len // 2 + 1]\n",
    "        imag = specs[:, self.fft_len // 2 + 1:]\n",
    "        spec_mags = torch.sqrt(real ** 2 + imag ** 2 + 1e-8)\n",
    "\n",
    "        spec_phase = torch.atan2(imag, real)\n",
    "        cspecs = torch.stack([real, imag], 1)\n",
    "        cspecs = cspecs[:, :, 1:]\n",
    "        '''\n",
    "        means = torch.mean(cspecs, [1,2,3], keepdim=True)\n",
    "        std = torch.std(cspecs, [1,2,3], keepdim=True )\n",
    "        normed_cspecs = (cspecs-means)/(std+1e-8)\n",
    "        out = normed_cspecs\n",
    "        '''\n",
    "\n",
    "        out = cspecs\n",
    "        encoder_out = []\n",
    "\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            out = layer(out)\n",
    "            #    print('encoder', out.size())\n",
    "            encoder_out.append(out)\n",
    "\n",
    "        batch_size, channels, dims, lengths = out.size()\n",
    "        out = out.permute(3, 0, 1, 2)\n",
    "        if lstm == 'complex':\n",
    "            r_rnn_in = out[:, :, :channels // 2]\n",
    "            i_rnn_in = out[:, :, channels // 2:]\n",
    "            r_rnn_in = torch.reshape(r_rnn_in, [lengths, batch_size, channels // 2 * dims])\n",
    "            i_rnn_in = torch.reshape(i_rnn_in, [lengths, batch_size, channels // 2 * dims])\n",
    "\n",
    "            r_rnn_in, i_rnn_in = self.enhance([r_rnn_in, i_rnn_in])\n",
    "\n",
    "            r_rnn_in = torch.reshape(r_rnn_in, [lengths, batch_size, channels // 2, dims])\n",
    "            i_rnn_in = torch.reshape(i_rnn_in, [lengths, batch_size, channels // 2, dims])\n",
    "            out = torch.cat([r_rnn_in, i_rnn_in], 2)\n",
    "        else:\n",
    "            # to [L, B, C, D]\n",
    "            out = torch.reshape(out, [lengths, batch_size, channels * dims])\n",
    "            out, _ = self.enhance(out)\n",
    "            out = self.tranform(out)\n",
    "            out = torch.reshape(out, [lengths, batch_size, channels, dims])\n",
    "\n",
    "        out = out.permute(1, 2, 3, 0)\n",
    "\n",
    "        if skip_type:  # use skip connection\n",
    "            for idx in range(len(self.decoder)):\n",
    "                out = complex_cat([out, encoder_out[-1 - idx]], 1)\n",
    "                out = self.decoder[idx](out)\n",
    "                out = out[..., 1:]  #\n",
    "        else:\n",
    "            for idx in range(len(self.decoder)):\n",
    "                out = self.decoder[idx](out)\n",
    "                out = out[..., 1:]\n",
    "\n",
    "        if self.masking_mode == 'Direct(None make)':\n",
    "            # for loss calculation\n",
    "            target_specs = self.stft(targets)\n",
    "            target_real = target_specs[:, :self.fft_len // 2 + 1]\n",
    "            target_imag = target_specs[:, self.fft_len // 2 + 1:]\n",
    "\n",
    "            # spectral mapping\n",
    "            out_real = out[:, 0]\n",
    "            out_imag = out[:, 1]\n",
    "            out_real = F.pad(out_real, [0, 0, 1, 0])\n",
    "            out_imag = F.pad(out_imag, [0, 0, 1, 0])\n",
    "\n",
    "            out_spec = torch.cat([out_real, out_imag], 1)\n",
    "\n",
    "            out_wav = self.istft(out_spec)\n",
    "            out_wav = torch.squeeze(out_wav, 1)\n",
    "            out_wav = torch.clamp_(out_wav, -1, 1)\n",
    "\n",
    "            return out_real, target_real, out_imag, target_imag, out_wav\n",
    "        else:\n",
    "            #    print('decoder', out.size())\n",
    "            mask_real = out[:, 0]\n",
    "            mask_imag = out[:, 1]\n",
    "            mask_real = F.pad(mask_real, [0, 0, 1, 0])\n",
    "            mask_imag = F.pad(mask_imag, [0, 0, 1, 0])\n",
    "\n",
    "            if self.masking_mode == 'E':\n",
    "                mask_mags = (mask_real ** 2 + mask_imag ** 2) ** 0.5\n",
    "                real_phase = mask_real / (mask_mags + 1e-8)\n",
    "                imag_phase = mask_imag / (mask_mags + 1e-8)\n",
    "                mask_phase = torch.atan2(\n",
    "                    imag_phase,\n",
    "                    real_phase\n",
    "                )\n",
    "\n",
    "                # mask_mags = torch.clamp_(mask_mags,0,100)\n",
    "                mask_mags = torch.tanh(mask_mags)\n",
    "                est_mags = mask_mags * spec_mags\n",
    "                est_phase = spec_phase + mask_phase\n",
    "                out_real = est_mags * torch.cos(est_phase)\n",
    "                out_imag = est_mags * torch.sin(est_phase)\n",
    "            elif self.masking_mode == 'C':\n",
    "                out_real, out_imag = real * mask_real - imag * mask_imag, real * mask_imag + imag * mask_real\n",
    "            elif self.masking_mode == 'R':\n",
    "                out_real, out_imag = real * mask_real, imag * mask_imag\n",
    "\n",
    "            out_spec = torch.cat([out_real, out_imag], 1)\n",
    "\n",
    "            out_wav = self.istft(out_spec)\n",
    "            out_wav = torch.squeeze(out_wav, 1)\n",
    "            out_wav = torch.clamp_(out_wav, -1, 1)\n",
    "\n",
    "            return out_real, out_imag, out_wav\n",
    "\n",
    "    def get_params(self, weight_decay=0.0):\n",
    "        # add L2 penalty\n",
    "        weights, biases = [], []\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                biases += [param]\n",
    "            else:\n",
    "                weights += [param]\n",
    "        params = [{\n",
    "            'params': weights,\n",
    "            'weight_decay': weight_decay,\n",
    "        }, {\n",
    "            'params': biases,\n",
    "            'weight_decay': 0.0,\n",
    "        }]\n",
    "        return params\n",
    "\n",
    "    def loss(self, estimated, target, real_spec=0, img_spec=0, perceptual=False):\n",
    "        if perceptual:\n",
    "            if perceptual == 'LMS':\n",
    "                clean_specs = self.stft(target)\n",
    "                clean_real = clean_specs[:, :self.fft_len // 2 + 1]\n",
    "                clean_imag = clean_specs[:, self.fft_len // 2 + 1:]\n",
    "                clean_mags = torch.sqrt(clean_real ** 2 + clean_imag ** 2 + 1e-7)\n",
    "\n",
    "                est_clean_mags = torch.sqrt(real_spec ** 2 + img_spec ** 2 + 1e-7)\n",
    "                return get_array_lms_loss(clean_mags, est_clean_mags)\n",
    "            elif perceptual == 'PMSQE':\n",
    "                return get_array_pmsqe_loss(target, estimated)\n",
    "        else:\n",
    "            if loss == 'MSE':\n",
    "                return F.mse_loss(estimated, target, reduction='mean')\n",
    "            elif loss == 'SDR':\n",
    "                return -sdr(target, estimated)\n",
    "            elif loss == 'SI-SNR':\n",
    "                return -(si_snr(estimated, target))\n",
    "            elif loss == 'SI-SDR':\n",
    "                return -(si_sdr(target, estimated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb2ed34-6eaf-4a03-b2e7-f978dd60df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCCRN().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e7d2aa8-1056-465f-b82e-de6bfa46829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_path = HOME_DIR+\"models/EXPERIMENT_NUMBER_11.22_DCCRN_SDR/chkpt_100.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2e1aed-a062-45be-9cb5-936e3254e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(chkpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f6017d-5716-441e-aee2-c8309e85f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df65bf86-cdf0-4d31-a354-60f346e5a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Test dataset>\n",
      "Load the data...\n"
     ]
    }
   ],
   "source": [
    "test_loader = create_dataloader(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b34e08-20e0-4232-914a-a7c896f717ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.0037, -0.0024, -0.0012,  ..., -0.0106, -0.0099, -0.0095],\n",
      "        [-0.0003, -0.0004, -0.0005,  ..., -0.0010, -0.0009, -0.0005],\n",
      "        [-0.0092, -0.0091, -0.0092,  ...,  0.0031,  0.0031,  0.0029],\n",
      "        ...,\n",
      "        [ 0.0084,  0.0084,  0.0082,  ..., -0.0008, -0.0005, -0.0002],\n",
      "        [-0.0046, -0.0079, -0.0110,  ..., -0.0023, -0.0020, -0.0017],\n",
      "        [ 0.0000,  0.0003,  0.0006,  ..., -0.0083, -0.0103, -0.0118]]), tensor([[-3.6926e-03, -2.4414e-03, -1.2207e-03,  ..., -1.0529e-02,\n",
      "         -1.0010e-02, -9.4299e-03],\n",
      "        [-2.4414e-04, -3.3569e-04, -5.7983e-04,  ..., -9.7656e-04,\n",
      "         -1.0071e-03, -5.1880e-04],\n",
      "        [-9.1858e-03, -9.0637e-03, -9.2468e-03,  ...,  2.9907e-03,\n",
      "          3.1433e-03,  2.8381e-03],\n",
      "        ...,\n",
      "        [ 8.3313e-03,  8.3923e-03,  8.1787e-03,  ..., -7.0190e-04,\n",
      "         -5.4932e-04, -6.1035e-05],\n",
      "        [-4.6082e-03, -7.9041e-03, -1.1047e-02,  ..., -2.3499e-03,\n",
      "         -1.9836e-03, -1.6785e-03],\n",
      "        [-1.2207e-04,  3.6621e-04,  5.1880e-04,  ..., -8.2703e-03,\n",
      "         -1.0345e-02, -1.1749e-02]])]\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(test_loader)\n",
    "data = next(batch_iterator)\n",
    "\n",
    "print(f\"{data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6b970-56a8-4e9d-b312-b609210ad917",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfeecf-3901-4ccd-bf69-f54b1466334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 820/4540: [========>.........................................] - ETA 676.6s"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_num = 0\n",
    "\n",
    "avg_pesq_score = 0\n",
    "avg_stoi_score = 0\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in Bar(test_loader):\n",
    "        batch_num += 1\n",
    "\n",
    "        # to cuda\n",
    "        inputs = inputs.float().to(DEVICE)\n",
    "        targets = targets.float().to(DEVICE)\n",
    "\n",
    "        _, _, outputs = model(inputs, targets)\n",
    "       \n",
    "\n",
    "        # estimate the output speech with pesq and stoi\n",
    "        estimated_wavs = outputs.cpu().detach().numpy()\n",
    "        clean_wavs = targets.cpu().detach().numpy()\n",
    "\n",
    "        pesq_score = cal_pesq(estimated_wavs, clean_wavs)\n",
    "        stoi_score = cal_stoi(estimated_wavs, clean_wavs)\n",
    "\n",
    "        # pesq: 0.1 better / stoi: 0.01 better\n",
    "        # for i in range(len(stoi)):\n",
    "        #     f_score.write('PESQ {:.6f} | STOI {:.6f}\\n'.format(pesq[i], stoi[i]))\n",
    "\n",
    "        # reshape for sum\n",
    "        pesq_score = np.reshape(pesq_score, (1, -1))\n",
    "        stoi_score = np.reshape(stoi_score, (1, -1))\n",
    "\n",
    "        avg_pesq_score += sum(pesq_score[0]) / len(inputs)\n",
    "        avg_stoi_score += sum(stoi_score[0]) / len(inputs)\n",
    "\n",
    "\n",
    "\n",
    "    # save the samples to tensorboard\n",
    "    # if epoch % 10 == 0:\n",
    "    #     writer.log_wav(inputs[0], targets[0], outputs[0], epoch)\n",
    "\n",
    "   \n",
    "    avg_pesq_score /= batch_num\n",
    "    avg_stoi_score /= batch_num\n",
    "    \n",
    "\n",
    "    print(f\"avg_pesq {avg_pesq_score} avg_stoi {avg_stoi_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54610c0-739f-496e-94d1-3e4c882f6d46",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9f5f0-bcc0-46ef-85b3-9035d7bbe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile_loader = DataLoader(dataset=WaveFile_Dataset(mode='test'), batch_size=batch, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa655e2-91e1-452a-b129-5f2641a8ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchfile_iterator = iter(testfile_loader)\n",
    "data = next(batchfile_iterator)\n",
    "\n",
    "print(f\"{data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1280e-6022-4d16-afd1-8751b38e6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data[0].float().to(DEVICE)\n",
    "targets = data[0].float().to(DEVICE)\n",
    "\n",
    "_, _, outputs = model(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528112b8-b7cb-4d8a-b3b5-3038058ec4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c62e7-be2b-4d1d-bb0a-d6caa794233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =inputs[0].cpu().numpy()\n",
    "sr = 8000\n",
    "\n",
    "# data = HOME_DIR+'kumc_lung_clinical_study/eval/8_Noisy.wav'\n",
    "# sr = 8000\n",
    "\n",
    "# y, sr = librosa.core.load(data, sr=sr)\n",
    "\n",
    "librosa.display.waveshow(y)\n",
    "\n",
    "D = np.abs(librosa.stft(y))**2\n",
    "S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='s',\n",
    "                         y_axis='log', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d8ca6-31f0-4e9a-b5d7-ea537f21eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =outputs[0].detach().cpu().numpy()\n",
    "sr = 8000\n",
    "\n",
    "# data = HOME_DIR+'kumc_lung_clinical_study/eval/8_Noisy.wav'\n",
    "# sr = 8000\n",
    "\n",
    "# y, sr = librosa.core.load(data, sr=sr)\n",
    "\n",
    "librosa.display.waveshow(y)\n",
    "\n",
    "D = np.abs(librosa.stft(y))**2\n",
    "S = librosa.feature.melspectrogram(S=D, sr=sr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_dB, x_axis='s',\n",
    "                         y_axis='log', sr=sr,\n",
    "                         fmax=8000, ax=ax)\n",
    "\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set(title='Mel-frequency spectrogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee144ab4-1741-4a63-8bed-6c258bb6a8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ss",
   "language": "python",
   "name": "ss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
